{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reference\n",
    "[uproot documentation](https://uproot.readthedocs.io/en/latest/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "interaction_dictionary = {}\n",
    "with open('interactions.csv') as f:\n",
    "    reader = csv.DictReader(f)\n",
    "    for row in reader:\n",
    "        key = int(row.pop('Idx'))\n",
    "        interaction = row.pop('Interaction')\n",
    "        interaction_dictionary[key] = interaction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_sequential(obj):\n",
    "    seq_events = np.zeros_like(obj.event)\n",
    "    seq_events[0] = obj.event[0]\n",
    "    seq_event = 0\n",
    "    seq_files = np.zeros_like(obj.event)\n",
    "    seq_files[0] = obj.event[0]\n",
    "    seq_file = 0\n",
    "    for i in range(1, len(obj.event)):\n",
    "        if obj.event[i] != obj.event[i - 1]:\n",
    "            seq_event += 1\n",
    "        if obj.event[i] < obj.event[i - 1]:\n",
    "            seq_file += 1\n",
    "        seq_events[i] = seq_event\n",
    "        seq_files[i] = seq_file\n",
    "    obj.event = seq_events\n",
    "    obj.file = seq_files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import uproot, numpy as np\n",
    "\n",
    "#Add a criteria for energy of each pfo\n",
    "#Add a criteria for adc for each plane\n",
    "#Add a criteria for true energy\n",
    "#Add a criteria to view CaloHits in each plane\n",
    "\n",
    "\n",
    "class MCValidation:\n",
    "    def __init__(self, filename, treename):\n",
    "        file = uproot.open(filename)\n",
    "        tree = file[treename]\n",
    "        self.event = tree['event'].array(library=\"np\") #linear list of events\n",
    "        self.orig_event = tree[\"event\"].array(library=\"np\") #actual event number\n",
    "        self.file = np.zeros_like(self.event)\n",
    "        self.mc_id = tree[\"mcId\"].array(library=\"np\") #number of mc particles in event\n",
    "        self.mc_energy = tree[\"mcEnergy\"].array(library=\"np\") #pdg code of particles in each event\n",
    "        self.mc_pdg = tree[\"mcPDG\"].array(library=\"np\") #pdg code of particles in each event\n",
    "        self.mc_tier = tree[\"mcTier\"].array(library=\"np\") #Which tier each event is folded back to\n",
    "        self.mc_nhits = tree[\"mcNHits\"].array(library=\"np\") #number of mc particles hits in event\n",
    "        is_nu_int = tree[\"isNuInteration\"].array(library=\"np\") #not functioning\n",
    "        is_cr_int = tree[\"isCosmicRay\"].array(library=\"np\") #not functioning\n",
    "        is_tb_int = tree[\"isTestBeam\"].array(library=\"np\") #not functioning\n",
    "        self.environment = np.full(is_nu_int.shape, \"??\") #not functioning\n",
    "        self.environment[np.where(is_nu_int)] = \"nu\" #not functioning\n",
    "        self.environment[np.where(is_cr_int)] = \"tb\" #not functioning\n",
    "        self.environment[np.where(is_tb_int)] = \"cr\" #not functioning\n",
    "        self.is_leading_lepton = tree[\"isLeadingLepton\"].array(library=\"np\")\n",
    "        self.is_michel = tree[\"isMichel\"].array(library=\"np\")\n",
    "        self.n_matches = tree[\"nMatches\"].array(library=\"np\")\n",
    "        self.reco_id_list = tree[\"recoIdVector\"].array(library=\"np\")\n",
    "        self.reco_nhits_list = tree[\"nRecoHitsVector\"].array(library=\"np\")\n",
    "        self.shared_nhits_list = tree[\"nSharedHitsVector\"].array(library=\"np\")\n",
    "        self.purity_adc_list = tree[\"purityAdcVector\"].array(library=\"np\")\n",
    "        self.purity_list_u = tree[\"purityVectorU\"].array(library=\"np\")\n",
    "        self.purity_list_v = tree[\"purityVectorV\"].array(library=\"np\")\n",
    "        self.purity_list_w = tree[\"purityVectorW\"].array(library=\"np\")\n",
    "        self.purity_adc_list_u = tree[\"purityAdcVectorU\"].array(library=\"np\")\n",
    "        self.purity_adc_list_v = tree[\"purityAdcVectorV\"].array(library=\"np\")\n",
    "        self.purity_adc_list_w = tree[\"purityAdcVectorW\"].array(library=\"np\")\n",
    "        self.completeness_adc_list = tree[\"completenessAdcVector\"].array(library=\"np\")\n",
    "        self.completeness_list_u = tree[\"completenessVectorU\"].array(library=\"np\")\n",
    "        self.completeness_list_v = tree[\"completenessVectorV\"].array(library=\"np\")\n",
    "        self.completeness_list_w = tree[\"completenessVectorW\"].array(library=\"np\")\n",
    "        self.completeness_adc_list_u = tree[\"completenessAdcVectorU\"].array(library=\"np\")\n",
    "        self.completeness_adc_list_v = tree[\"completenessAdcVectorV\"].array(library=\"np\")\n",
    "        self.completeness_adc_list_w = tree[\"completenessAdcVectorW\"].array(library=\"np\")\n",
    "        self.pc_metric = self.purity_adc_list * self.completeness_adc_list\n",
    "        self.vtx_dx = tree[\"vtxDx\"].array(library=\"np\")\n",
    "        self.vtx_x = tree[\"vtxX\"].array(library=\"np\")\n",
    "        self.vtx_dy = tree[\"vtxDy\"].array(library=\"np\")\n",
    "        self.vtx_y = tree[\"vtxY\"].array(library=\"np\")\n",
    "        self.vtx_dz = tree[\"vtxDz\"].array(library=\"np\")\n",
    "        self.vtx_z = tree[\"vtxZ\"].array(library=\"np\")\n",
    "        self.vtx_dr = tree[\"vtxDr\"].array(library=\"np\")\n",
    "        file.close()\n",
    "        make_sequential(self)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "def save_plot(fig, filename, subdir=None):\n",
    "    if subdir is None:\n",
    "        subdir = \"\"\n",
    "    elif subdir.startswith(\"/\"):\n",
    "        subdir = subdir[1:]\n",
    "        \n",
    "    if not os.path.exists('images'):\n",
    "        os.mkdir('images')\n",
    "    for img_type in [ \"png\", \"svg\", \"eps\" ]:\n",
    "        if not os.path.exists(f'images/{img_type}'):\n",
    "            os.mkdir(f'images/{img_type}')\n",
    "        if not os.path.exists(f'images/{img_type}/{subdir}'):\n",
    "            os.mkdir(f'images/{img_type}/{subdir}')\n",
    "        fig.savefig(f'images/{img_type}/{subdir}/{filename}.{img_type}', dpi=200)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reading data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#For reading singular arrays\n",
    "\n",
    "validation = MCValidation(\"hierarchy_validation_MC_test.root\", \"events\")\n",
    "x = validation.vtx_x\n",
    "print(np.unique(x))\n",
    "print(np.size(x))\n",
    "print(\"Here:\", np.min(x))\n",
    "for i in range(0, np.size(x)):\n",
    "    print(x[i])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#For reading arrays with multiple entries\n",
    "\n",
    "x = validation.mc_tier\n",
    "print(np.size(x))\n",
    "for i in range(0, np.size(x)):\n",
    "    size = np.size(x[i])\n",
    "    print(size)\n",
    "    if size > 1:\n",
    "        j = 0\n",
    "        while j < size:\n",
    "            print(\"here\")\n",
    "            print(x[i][j])\n",
    "            j += 1\n",
    "    else:\n",
    "        print(x[i])\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Printing vertex info\n",
    "validation = MCValidation(\"hierarchy_validation_MC_22MeV.root\", \"events\")\n",
    "n_vectors = np.size(validation.vtx_dx)\n",
    "for i in range(0, n_vectors):\n",
    "    vertex_vector = []\n",
    "    vertex_vector.append(validation.vtx_dx[i])\n",
    "    vertex_vector.append(validation.vtx_dy[i])\n",
    "    vertex_vector.append(validation.vtx_dz[i])\n",
    "    vertex_vector.append(validation.vtx_dr[i])\n",
    "    print(vertex_vector)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Plotting coordinates of the vertices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "cut = 0.03\n",
    "\n",
    "print(\"\\033[1m\" \"mcEnergy\")\n",
    "\n",
    "\n",
    "validation = MCValidation(\"hierarchy_validation_MC_test.root\", \"events\")\n",
    "x = validation.mc_energy\n",
    "max_value = np.max(x)\n",
    "bins = np.linspace(0, 0.03, 200)\n",
    "idx = np.where(x < cut)\n",
    "x[idx]\n",
    "plt.hist(x[idx], density=False, bins=bins, histtype = 'step' )\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "print(\"\\033[1m\" \"X Coordinate\")\n",
    "\n",
    "validation = MCValidation(\"hierarchy_validation_MC_test.root\", \"events\")\n",
    "x = validation.vtx_x\n",
    "max_value = np.max(x)\n",
    "bins = np.linspace(0, max_value, 100)\n",
    "idx = np.where(x != 0)\n",
    "x[idx]\n",
    "plt.hist(x[idx], density=False, bins=bins, histtype = 'step' )\n",
    "\n",
    "plt.show()\n",
    "\n",
    "print(\"\\033[1m\" \"Y Coordinate\")\n",
    "\n",
    "y = validation.vtx_y\n",
    "max_value = np.max(y)\n",
    "bins = np.linspace(0, max_value, 100)\n",
    "idx = np.where(y != 0)\n",
    "y[idx]\n",
    "plt.hist(y[idx], density=False, bins=bins, histtype = 'step' )\n",
    "\n",
    "plt.show()\n",
    "\n",
    "print(\"\\033[1m\" \"Z Coordinate\")\n",
    "\n",
    "z = validation.vtx_z\n",
    "max_value = np.max(z)\n",
    "bins = np.linspace(0, max_value, 100)\n",
    "idx = np.where(z != 0)\n",
    "z[idx]\n",
    "plt.hist(z[idx], density=False, bins=bins, histtype = 'step' )\n",
    "\n",
    "\n",
    "plt.show()\n",
    "\n",
    "\n",
    "print(\"\\033[1m\" \"Dx Coordinate\")\n",
    "\n",
    "validation = MCValidation(\"hierarchy_validation_MC_test.root\", \"events\")\n",
    "x = validation.vtx_dx\n",
    "max_value = np.max(x)\n",
    "bins = np.linspace(0, max_value, 100)\n",
    "idx = np.where(x != 0)\n",
    "x[idx]\n",
    "plt.hist(x[idx], density=False, bins=bins, histtype = 'step' )\n",
    "\n",
    "plt.show()\n",
    "\n",
    "print(\"\\033[1m\" \"Dy Coordinate\")\n",
    "\n",
    "y = validation.vtx_dy\n",
    "max_value = np.max(y)\n",
    "bins = np.linspace(0, max_value, 100)\n",
    "idx = np.where(y != 0)\n",
    "y[idx]\n",
    "plt.hist(y[idx], density=False, bins=bins, histtype = 'step' )\n",
    "\n",
    "plt.show()\n",
    "\n",
    "print(\"\\033[1m\" \"Dz Coordinate\")\n",
    "\n",
    "z = validation.vtx_dz\n",
    "max_value = np.max(z)\n",
    "bins = np.linspace(0, max_value, 100)\n",
    "idx = np.where(z != 0)\n",
    "z[idx]\n",
    "plt.hist(z[idx], density=False, bins=bins, histtype = 'step' )\n",
    "\n",
    "\n",
    "plt.show()\n",
    "\n",
    "\n",
    "print(\"\\033[1m\" \"Dr Coordinate\")\n",
    "\n",
    "z = validation.vtx_dr\n",
    "max_value = np.max(z)\n",
    "bins = np.linspace(0, max_value, 100)\n",
    "idx = np.where(z != 0)\n",
    "z[idx]\n",
    "plt.hist(z[idx], density=False, bins=bins, histtype = 'step' )\n",
    "\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Plotting mc_nhits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import statistics as st\n",
    "import matplotlib.pyplot as plt\n",
    "#All_energies = [5, 6, 7, 8, 9,13,14,16,17,20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30]\n",
    "All_energies = list(range(5, 10)) + [13, 14, 16, 17] + list(range(20, 31))\n",
    "\n",
    "def make_linear(validation, size_criteria):\n",
    "    mc_pdg = MCValidation(\"hierarchy_validation_MC_\" + str(file_energy) + \"MeV.root\", \"events\").mc_pdg\n",
    "    temp_array_phot = []\n",
    "    temp_array_elec = []\n",
    "    for i in range(0, size_criteria):\n",
    "        if mc_pdg[i] == 22: \n",
    "            subdivision_size = np.size(validation[i])\n",
    "            x = 0\n",
    "            empty = 0\n",
    "            if subdivision_size == 0:\n",
    "                empty += 1\n",
    "            if subdivision_size == 1:\n",
    "                if file_criteria == \"mc_nhits\" or \"mc_pdg\":\n",
    "                    temp_array_phot.append(validation[i])\n",
    "                else: \n",
    "                    temp_array_phot.append(validation[i][0])\n",
    "            if subdivision_size > 1:\n",
    "                while x < subdivision_size:\n",
    "                    temp_array_phot.append(validation[i][x])\n",
    "                    x += 1\n",
    "        if mc_pdg[i] == 11: \n",
    "            subdivision_size = np.size(validation[i])\n",
    "            x = 0\n",
    "            empty = 0\n",
    "            if suhttp://localhost:8888/notebooks/hierarchy.ipynb#Plotting-mc_nhitsbdivision_size == 0:\n",
    "                empty += 1\n",
    "            if subdivision_size == 1:\n",
    "                if file_criteria == \"mc_nhits\" or \"mc_pdg\":\n",
    "                    temp_array_elec.append(validation[i])\n",
    "                else: \n",
    "                    temp_array_elec.append(validation[i][0])\n",
    "            if subdivision_size > 1:\n",
    "                while x < subdivision_size:\n",
    "                    temp_array_elec.append(validation[i][x])\n",
    "                    x += 1\n",
    "    return temp_array_phot, temp_array_elec\n",
    "\n",
    "#size_difference = int(make_linear(full_criteria)[1]) - int(size_criteria)\n",
    "7717\n",
    "#Compatible criteria \"reco_nhits_list\", \"shared_nhits_list\", \"reco_id_list\"\n",
    "\n",
    "file_criteria = \"mc_nhits\" #From list of keys above\n",
    "file_energy = 5\n",
    "Loop_for_all_energies = True\n",
    "print_array = False\n",
    "plot = True\n",
    "\n",
    "if Loop_for_all_energies == True:\n",
    "    Energies = All_energies\n",
    "else:\n",
    "    Energies = [file_energy]\n",
    "for i in Energies:\n",
    "    file_energy = i #In MeV, which energy file should be read\n",
    "    validation = MCValidation(\"hierarchy_validation_MC_\" + str(file_energy) + \"MeV.root\", \"events\")\n",
    "    full_criteria = eval(\"validation.\" + file_criteria)\n",
    "    nice_array = make_linear(full_criteria, np.size(full_criteria))\n",
    "    size_difference = np.size(nice_array[0]) + np.size(nice_array[1]) - np.size(full_criteria) \n",
    "    print(\"\\033[1m\" + file_criteria + \"\\033[0m\" + \" is as follows:\")\n",
    "    print(\"\\n\")\n",
    "    print(\"The length of \" + file_criteria + \" is \" + \"\\033[1m\" + str(np.size(full_criteria)) + \"\\033[0m\")\n",
    "    print(\"The number of empty events recorded \" + \"\\033[1m\" + str(abs(size_difference)) + \"\\033[0m\" )\n",
    "    print(\"\\n\")\n",
    "    print(\"The mean number of photon hits for\", i, \"MeV is\", st.mean(nice_array[0]))\n",
    "    print(\"The median number of photon hits for\", i, \"MeV is\",st.median(nice_array[0]))\n",
    "    print(\"\\n\")\n",
    "    print(\"The mean number of electron hits for\", i, \"MeV is\", st.mean(nice_array[1]))\n",
    "    print(\"The median number of electron hits for\", i, \"MeV is\",st.median(nice_array[1]))\n",
    "    \n",
    "    if print_array == True:\n",
    "        print(nice_array)\n",
    "    \n",
    "    #Plotting\n",
    "    if plot == True:\n",
    "        #max_value = np.max(nice_array)\n",
    "        bins = np.linspace(0, 100 , 100)\n",
    "        #bins = np.linspace(0, max_value, max_value)\n",
    "        plt.hist(nice_array[0], density=False, bins=bins, histtype = 'step', label = \"photon\")\n",
    "        plt.hist(nice_array[1], density=False, bins=bins, histtype = 'step', label = \"electron\")\n",
    "        plt.legend()\n",
    "        #plt.xlim(3, np.max(nice_array))\n",
    "        plt.show()\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Plotting reco_nhits_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import statistics as st\n",
    "import matplotlib.pyplot as plt\n",
    "#All_energies = [5, 6, 7, 8, 9,13,14,16,17,20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30]\n",
    "All_energies = list(range(5, 10)) + [13, 14, 16, 17] + list(range(20, 31))\n",
    "\n",
    "def make_linear(validation, file_criteria, size_criteria):\n",
    "    temp_array = []\n",
    "    #emp_array_elec = []\n",
    "    for i in range(0, size_criteria):\n",
    "        subdivision_size = np.size(validation[i])\n",
    "        x = 0\n",
    "        empty = 0\n",
    "        if subdivision_size == 0:\n",
    "            empty += 1\n",
    "        if subdivision_size == 1:\n",
    "            temp_array.append(validation[i][0])\n",
    "        if subdivision_size > 1:\n",
    "            while x < subdivision_size:\n",
    "                temp_array.append(validation[i][x])\n",
    "                x += 1\n",
    "    return temp_array\n",
    "\n",
    "#size_difference = int(make_linear(full_criteria)[1]) - int(size_criteria)\n",
    "\n",
    "#Compatible criteria \"reco_nhits_list\", \"shared_nhits_list\", \"reco_id_list\"\n",
    "\n",
    "file_criteria = \"reco_nhits_list\" #From list of keys above\n",
    "file_energy = 5\n",
    "Loop_for_all_energies = True\n",
    "print_array = False\n",
    "plot = True\n",
    "\n",
    "if Loop_for_all_energies == True:\n",
    "    Energies = All_energies\n",
    "else:\n",
    "    Energies = [file_energy]\n",
    "for i in Energies:\n",
    "    file_energy = i #In MeV, which energy file should be read\n",
    "    validation = MCValidation(\"hierarchy_validation_MC_\" + str(file_energy) + \"MeV.root\", \"events\")\n",
    "    full_criteria = eval(\"validation.\" + file_criteria)\n",
    "    #if full_criteria == \"mc_nhits\":\n",
    "     #   nice_array = np.concatenate(full_criteria).flatten()\n",
    "    #if full_criteria = \"reco_nhits_list\"\n",
    "    nice_array = make_linear(full_criteria, file_criteria, np.size(full_criteria))\n",
    "    size_difference = np.size(nice_array) - np.size(full_criteria) \n",
    "    print(\"\\033[1m\" + file_criteria + \"\\033[0m\" + \" is as follows:\")\n",
    "    print(\"\\n\")\n",
    "    #print(np.min(nice_array))\n",
    "    print(\"The length of \" + file_criteria + \" is \" + \"\\033[1m\" + str(np.size(full_criteria)) + \"\\033[0m\")\n",
    "    print(\"The number of empty events recorded \" + \"\\033[1m\" + str(abs(size_difference)) + \"\\033[0m\" )\n",
    "    #print(\"The mean number of reco hits for\", i, \"MeV is\", st.mean(nice_array))\n",
    "    #print(\"The median number of reco hits for\", i, \"MeV is\",st.median(nice_array))\n",
    "    \n",
    "    if print_array == True:\n",
    "        print(nice_array)\n",
    "    \n",
    "    #Plotting\n",
    "    if plot == True:\n",
    "        max_value = np.max(nice_array)\n",
    "        bins = np.linspace(0, 100 , 100)\n",
    "        #bins = np.linspace(0, max_value, max_value)\n",
    "        plt.hist(nice_array, density=False, bins=bins, histtype = 'step' )\n",
    "        #plt.xlim(3, np.max(nice_array))\n",
    "        plt.show()\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Individual Efficiency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "purity_cut = 0\n",
    "completeness_cut = 0.65\n",
    "Total_events = 5000\n",
    "\n",
    "def get_efficiency(validation, pdg, signed=False, is_michel=False, tier=None):\n",
    "    mc_pdg = abs(validation.mc_pdg) if not signed else validation.mc_pdg\n",
    "    pdg = abs(pdg) if not signed else pdg\n",
    "    particle_filter = mc_pdg == pdg\n",
    "    if is_michel:\n",
    "        particle_filter = particle_filter & (validation.is_michel)\n",
    "    if tier:\n",
    "        particle_filter = particle_filter & (validation.mc_tier == tier)\n",
    "\n",
    "    mc_idx = np.where(particle_filter)\n",
    "\n",
    "    #mc_purity = np.array([val[0] if len(val) > 0 else -1 for val in validation.purity_list])\n",
    "    #mc_completeness = np.array([val[0] if len(val) > 0 else -1 for val in validation.completeness_list])\n",
    "    # adjust this to just look at the quality tag\n",
    "    #mc_good_match = np.where((particle_filter) & (validation.n_matches == 1) & \\\n",
    "    #                         (mc_purity > purity_cut) & \\\n",
    "    #                         (mc_completeness > completeness_cut))\n",
    "    mc_good_match = np.where((particle_filter) & (validation.n_matches == 1))\n",
    "    mc_no_match = np.where((particle_filter) & (validation.n_matches == 0))\n",
    "    # adjust this to look at quality tag\n",
    "    #mc_poor_match = np.where((particle_filter) & (validation.n_matches > 1))\n",
    "    mc_poor_match = np.where((particle_filter) & (validation.n_matches > 0) & (validation.n_matches != 1))\n",
    "\n",
    "    n_good_matches = len(mc_good_match[0])\n",
    "    n_no_matches = len(mc_no_match[0])\n",
    "    n_poor_matches = len(mc_poor_match[0])\n",
    "\n",
    "    return np.array([n_good_matches, n_poor_matches, n_no_matches])\n",
    "\n",
    "def get_all_efficiencies(validation, michel_tag=False, tier=None):\n",
    "    photon_matches = get_efficiency(validation, 22, tier=tier)\n",
    "    proton_matches = get_efficiency(validation, 2212, tier=tier)\n",
    "    muon_matches = get_efficiency(validation, 13, tier=tier)\n",
    "    electron_matches = get_efficiency(validation, 11, is_michel=michel_tag, tier=tier)\n",
    "    pion_matches = get_efficiency(validation, 211, tier=tier)\n",
    "    kaon_matches = get_efficiency(validation, 321, tier=tier)\n",
    "    photon_eff = photon_matches / photon_matches.sum() if photon_matches.sum() else np.zeros_like(photon_matches)\n",
    "    proton_eff = proton_matches / proton_matches.sum() if proton_matches.sum() else np.zeros_like(proton_matches)\n",
    "    muon_eff = muon_matches / muon_matches.sum() if muon_matches.sum() else np.zeros_like(muon_matches)\n",
    "    electron_eff = electron_matches / electron_matches.sum() if electron_matches.sum() else np.zeros_like(electron_matches)\n",
    "    pion_eff = pion_matches / pion_matches.sum() if pion_matches.sum() else np.zeros_like(pion_matches)\n",
    "    kaon_eff = kaon_matches / kaon_matches.sum() if kaon_matches.sum() else np.zeros_like(kaon_matches)\n",
    "    electron_error = [math.sqrt(abs((electron_eff[0] * (electron_eff[0] - 1))/ electron_matches.sum()))]\n",
    "    all_eff = np.concatenate((muon_eff[:,None], proton_eff[:,None], pion_eff[:,None], \\\n",
    "                                  kaon_eff[:,None], photon_eff[:,None], electron_eff[:,None],), \\\n",
    "                                axis=1)\n",
    "    return all_eff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labelsize=14\n",
    "titlesize=18\n",
    "\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "from cycler import cycler\n",
    "\n",
    "def plot_efficiencies(all_eff, file_prefix, tier=None):\n",
    "    particle_labels = ['\\u03bc', 'p', '\\u03c0', '\\u03ba', '\\u03b3', 'e']\n",
    "    metric_labels = ['Good match', 'Poor match', 'Unmatched']\n",
    "    width = 0.5       # the width of the bars: can also be len(x) sequence\n",
    "\n",
    "    fig, ax = plt.subplots(figsize=(12,8))\n",
    "\n",
    "    # good\n",
    "    ax.bar(particle_labels, all_eff[0], width, label=metric_labels[0], color='green')\n",
    "    # poor\n",
    "    ax.bar(particle_labels, all_eff[1], width, label=metric_labels[1], bottom=all_eff[0], color='orange')\n",
    "    # unmatched\n",
    "    ax.bar(particle_labels, all_eff[2], width, label=metric_labels[2], bottom=all_eff[0] + all_eff[1], color='red')\n",
    "\n",
    "    ax.tick_params(axis='x', labelsize=labelsize)\n",
    "    ax.tick_params(axis='y', labelsize=labelsize)\n",
    "    ax.set_xlabel(\"particle\", fontsize=titlesize)\n",
    "    ax.set_ylabel('fraction', fontsize=titlesize)\n",
    "    ax.set_title('Reconstruction efficiency', fontsize=titlesize)\n",
    "\n",
    "    ax.legend(fontsize=titlesize)\n",
    "\n",
    "    fig.tight_layout()\n",
    "    plt.show()\n",
    "    if tier is None:\n",
    "        file_suffix = \"all\"\n",
    "    else:\n",
    "        file_suffix = f\"tier_{tier}\"\n",
    "    save_plot(fig, f'{file_prefix}_{file_suffix}_eff', subdir=\"efficiencies\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_eff(filename, treename, plot, michel_tag=False, tier=None):\n",
    "    validation = MCValidation(filename, treename)\n",
    "    all_eff = get_all_efficiencies(validation, michel_tag=michel_tag, tier=tier)\n",
    "    file_prefix = \"\"\n",
    "    if plot == True:\n",
    "        print(\"Running: \", filename)\n",
    "        print(\"Good Electrons:\", all_eff[0][5])\n",
    "        print(\"Poor Electrons:\", all_eff[1][5])\n",
    "        print(\"No Electrons:\", all_eff[2][5])\n",
    "        print(\"Good Photons:\", all_eff[0][4])\n",
    "        print(\"Poor Photons:\", all_eff[1][4])\n",
    "        print(\"No Photons:\", all_eff[2][4])\n",
    "        if \"stm\" in filename:\n",
    "            file_prefix += \"streamed\"\n",
    "        else:\n",
    "            file_prefix += \"standard\"\n",
    "        if \"unfolded\" in filename:\n",
    "            file_prefix += \"unfolded\"\n",
    "        else:\n",
    "            file_prefix += \"folded\"\n",
    "        plot_efficiencies(all_eff, file_prefix, tier)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "All_energies = [5, 6, 7, 8, 9, 13, 14, 16, 17, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30]\n",
    "\n",
    "\n",
    "def run_all(indx1, indx2, plot = False):\n",
    "    xaxis_elec_eff = []\n",
    "    yaxis_elec_eff = []\n",
    "    for i in All_energies:\n",
    "        filename = \"hierarchy_validation_MC_\" + str(i) + \"MeV.root\"\n",
    "        treename = \"events\"\n",
    "        validation = MCValidation(filename, treename)\n",
    "        all_info = get_all_efficiencies(validation)\n",
    "        run_eff(filename, treename, plot)\n",
    "        xaxis_elec_eff.append(i)\n",
    "        yaxis_elec_eff.append(all_info[indx1][indx2])\n",
    "    return xaxis_elec_eff, yaxis_elec_eff\n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "print(run_all(0, 5, False))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Combined Efficiency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.tick_params(axis='x', labelsize=labelsize)\n",
    "plt.tick_params(axis='y', labelsize=labelsize)\n",
    "plt.xlabel(\"Monoenergetic energy sample MeV\", fontsize=titlesize)\n",
    "plt.ylabel('Fraction', fontsize=titlesize)\n",
    "plt.title('Reconstruction efficiency', fontsize=titlesize)\n",
    "plt.plot(run_all(0, 5, False)[0], run_all(0, 5, False)[1], label = \"electron\")\n",
    "plt.plot(run_all(0, 4, False)[0], run_all(0, 4, False)[1], label = \"photon\")\n",
    "# plt.plot(run_all(0, 1, False)[0], run_all(0, 1, False)[1], label = \"proton\")\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Purity and Completeness"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_metric(metric_type, validation, pdg, leading_lepton=False, weighted=False, view=None, signed=False,\n",
    "               is_michel=False, tier=None):\n",
    "    mc_pdg = validation.mc_pdg # if not signed else validation.mc_pdg\n",
    "    pdg = abs(pdg) if not signed else pdg\n",
    "    particle_filter = mc_pdg == pdg\n",
    "    if is_michel:\n",
    "        particle_filter = particle_filter & (validation.is_michel)\n",
    "    if tier:\n",
    "        particle_filter = particle_filter & (validation.mc_tier == tier)\n",
    "    \n",
    "    if not leading_lepton:\n",
    "        idx = np.where((particle_filter) & (validation.n_matches >= 1))\n",
    "    else:\n",
    "        idx = np.where((particle_filter) & (validation.n_matches >= 1) & (validation.is_leading_lepton))\n",
    "\n",
    "    if not weighted:\n",
    "        if metric_type == \"purity\":\n",
    "            if not view:\n",
    "                metric = validation.purity_list[idx]\n",
    "            elif view.lower() == \"u\":\n",
    "                metric = validation.purity_list_u[idx]\n",
    "            elif view.lower() == \"v\":\n",
    "                metric = validation.purity_list_v[idx]\n",
    "            elif view.lower() == \"w\":\n",
    "                metric = validation.purity_list_w[idx]\n",
    "            else:\n",
    "                raise Exception(f\"Unknown view {view.upper()}\")\n",
    "        elif metric_type == \"completeness\":\n",
    "            if not view:\n",
    "                metric = validation.completeness_list[idx]\n",
    "            elif view.lower() == \"u\":\n",
    "                metric = validation.completeness_list_u[idx]\n",
    "            elif view.lower() == \"v\":\n",
    "                metric = validation.completeness_list_v[idx]\n",
    "            elif view.lower() == \"w\":\n",
    "                metric = validation.completeness_list_w[idx]\n",
    "            else:\n",
    "                raise Exception(f\"Unknown view {view.upper()}\")\n",
    "        else:\n",
    "            if not view:\n",
    "                metric = validation.pc_metric[idx]\n",
    "            else:\n",
    "                raise Exception(f\"Unknown view {view.upper()}\")\n",
    "    else:\n",
    "        if metric_type == \"purity\":\n",
    "            if not view:\n",
    "                metric = validation.purity_adc_list[idx]\n",
    "            elif view.lower() == \"u\":\n",
    "                metric = validation.purity_adc_list_u[idx]\n",
    "            elif view.lower() == \"v\":\n",
    "                metric = validation.purity_adc_list_v[idx]\n",
    "            elif view.lower() == \"w\":\n",
    "                metric = validation.purity_adc_list_w[idx]\n",
    "            else:\n",
    "                raise Exception(f\"Unknown view {view.upper()}\")\n",
    "        elif metric_type == \"completeness\":\n",
    "            if not view:\n",
    "                metric = validation.completeness_adc_list[idx]\n",
    "            elif view.lower() == \"u\":\n",
    "                metric = validation.completeness_adc_list_u[idx]\n",
    "            elif view.lower() == \"v\":\n",
    "                metric = validation.completeness_adc_list_v[idx]\n",
    "            elif view.lower() == \"w\":\n",
    "                metric = validation.completeness_adc_list_w[idx]\n",
    "            else:\n",
    "                raise Exception(f\"Unknown view {view.upper()}\")\n",
    "        else:\n",
    "            if not view:\n",
    "                metric = validation.pc_metric[idx]\n",
    "            else:\n",
    "                raise Exception(f\"Unknown view {view.upper()}\")\n",
    "\n",
    "    return metric.flatten()\n",
    "\n",
    "def get_purity(validation, pdg, leading_lepton=False, weighted=False, view=None, signed=False,\n",
    "               is_michel=False, tier=None):\n",
    "    return get_metric(\"purity\", validation, pdg, leading_lepton, weighted, view, signed, is_michel, tier)\n",
    "\n",
    "def get_completeness(validation, pdg, leading_lepton=False, weighted=False, view=None, signed=False,\n",
    "                     is_michel=False, tier=None):\n",
    "    return get_metric(\"completeness\", validation, pdg, leading_lepton, weighted, view, signed, is_michel, tier)\n",
    "\n",
    "def get_pc_metric(validation, pdg, leading_lepton=False, weighted=False, view=None, signed=False,\n",
    "                  is_michel=False, tier=None):\n",
    "    return get_metric(\"pc_metric\", validation, pdg, leading_lepton, weighted, None, signed, is_michel, tier)\n",
    "\n",
    "def get_all_purities(validation, weighted=False, view=None, michel_tag=False, tier=None):\n",
    "    muon_purity = get_purity(validation, 13, weighted=weighted, view=view, tier=tier)\n",
    "    proton_purity = get_purity(validation, 2212, weighted=weighted, view=view, tier=tier)\n",
    "    pion_purity = get_purity(validation, 211, weighted=weighted, view=view, tier=tier)\n",
    "    kaon_purity = get_purity(validation, 321, weighted=weighted, view=view, tier=tier)\n",
    "    photon_purity = get_purity(validation, 22, weighted=weighted, view=view, tier=tier)\n",
    "    electron_purity = get_purity(validation, 11, weighted=weighted, view=view, is_michel=michel_tag, tier=tier)\n",
    "    all_purity = [muon_purity, proton_purity, pion_purity, kaon_purity,\n",
    "                                 photon_purity, electron_purity]\n",
    "    return all_purity\n",
    "\n",
    "def get_all_completenesses(validation, weighted=False, view=None, michel_tag=False, tier=None):\n",
    "    muon_completeness = get_completeness(validation, 13, weighted=weighted, view=view, tier=tier)\n",
    "    proton_completeness = get_completeness(validation, 2212, weighted=weighted, view=view, tier=tier)\n",
    "    pion_completeness = get_completeness(validation, 211, weighted=weighted, view=view, tier=tier)\n",
    "    kaon_completeness = get_completeness(validation, 321, weighted=weighted, view=view, tier=tier)\n",
    "    photon_completeness = get_completeness(validation, 22, weighted=weighted, view=view, tier=tier)\n",
    "    electron_completeness = get_completeness(validation, 11, weighted=weighted, view=view, is_michel=michel_tag, tier=tier)\n",
    "    all_completeness = [muon_completeness, proton_completeness, pion_completeness,\n",
    "                                       kaon_completeness, photon_completeness, electron_completeness]\n",
    "    return all_completeness\n",
    "\n",
    "def get_all_pc_metrics(validation, weighted=False, michel_tag=False, tier=None):\n",
    "    muon_purity = get_pc_metric(validation, 13, weighted=weighted, view=None, tier=tier)\n",
    "    proton_purity = get_pc_metric(validation, 2212, weighted=weighted, view=None, tier=tier)\n",
    "    pion_purity = get_pc_metric(validation, 211, weighted=weighted, view=None, tier=tier)\n",
    "    kaon_purity = get_pc_metric(validation, 321, weighted=weighted, view=None, tier=tier)\n",
    "    photon_purity = get_pc_metric(validation, 22, weighted=weighted, view=None, tier=tier)\n",
    "    electron_purity = get_pc_metric(validation, 11, weighted=weighted, view=None, is_michel=michel_tag, tier=tier)\n",
    "    all_purity = [muon_purity, proton_purity, pion_purity, kaon_purity,\n",
    "                                 photon_purity, electron_purity]\n",
    "    return all_purity\n",
    "\n",
    "def get_leading_lepton_purities(validation, weighted=False):\n",
    "    muon_purity = get_purity(validation, 13, leading_lepton=True, weighted=weighted)\n",
    "    electron_purity = get_purity(validation, 11, leading_lepton=True, weighted=weighted)\n",
    "    all_purity = [muon_purity, electron_purity]\n",
    "    return all_purity\n",
    "\n",
    "def get_leading_lepton_completenesses(validation, weighted=False):\n",
    "    muon_completeness = get_completeness(validation, 13, leading_lepton=True, weighted=weighted)\n",
    "    electron_completeness = get_completeness(validation, 11, leading_lepton=True, weighted=weighted)\n",
    "    all_completeness = [muon_completeness, electron_completeness]\n",
    "    return all_completeness\n",
    "\n",
    "def get_leading_lepton_pc_metric(validation, weighted=False):\n",
    "    muon_purity = get_pc_metric(validation, 13, leading_lepton=True, weighted=weighted)\n",
    "    electron_purity = get_pc_metric(validation, 11, leading_lepton=True, weighted=weighted)\n",
    "    all_purity = [muon_purity, electron_purity]\n",
    "    return all_purity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(validation.mc_pdg)\n",
    "validation = MCValidation(\"hierarchy_validation_MC_16MeV.root\", \"events\")\n",
    "value1 = get_all_pc_metrics(validation)\n",
    "print(value1)\n",
    "print(value1[5])\n",
    "print(len(value1[5]))\n",
    "print(float(value1[5][80]))\n",
    "value10 = []\n",
    "for i in range(0,1316):\n",
    "    value10.append(float(value1[5][i][0]))\n",
    "    try:\n",
    "        value10.append(float(value1[5][i][1]))\n",
    "    except IndexError:\n",
    "        continue\n",
    "plot_metric(value10, value10, \"PC\", \"all\", \"pc\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_metric(values1, values2, labels, particle, metric, leading_lepton=False, tier=None, weighted=False, view=None, log=True):\n",
    "    if not leading_lepton:\n",
    "        particle_labels = ['\\u03bc', 'p', '\\u03c0', '\\u03ba', '\\u03b3', 'e']\n",
    "    else:\n",
    "        particle_labels = ['\\u03bc', 'e']\n",
    "\n",
    "    fig, ax = plt.subplots(figsize=(12,8))\n",
    "    \n",
    "    n_bins = 20\n",
    "    \n",
    "    bins = np.linspace(0, 1, n_bins + 1)\n",
    "\n",
    "    weights1 = np.ones_like(values1) / len(values1)\n",
    "    ax.hist(values1, bins=bins, weights=weights1, histtype='step', label=labels[0])\n",
    "    weights2 = np.ones_like(values2) / len(values2)\n",
    "    ax.hist(values2, bins=bins, weights=weights2, histtype='step', label=labels[1])\n",
    "    \n",
    "    ax.tick_params(axis='x', labelsize=labelsize)\n",
    "    ax.tick_params(axis='y', labelsize=labelsize)\n",
    "    ax.set_xlabel(f\"{metric}\", fontsize=titlesize)\n",
    "    ax.set_ylabel('event fraction', fontsize=titlesize)\n",
    "    \n",
    "    title = particle.title() + \" \" + metric.title()\n",
    "    if view:\n",
    "        title += f' {view.upper()}'\n",
    "    if weighted:\n",
    "        title += ' (ADC weighted)'\n",
    "    ax.set_title(f'{title}', fontsize=titlesize)\n",
    "    ax.legend(fontsize=titlesize, loc='upper left')\n",
    "\n",
    "    if log:\n",
    "        plt.yscale('log')\n",
    "    limits = (ax.get_ylim()[0], 1)\n",
    "    ax.set_ylim(limits)\n",
    "\n",
    "    fig.tight_layout()\n",
    "    plt.show()\n",
    "    filename = f'{particle.lower()}_{metric.lower()}'\n",
    "    if weighted:\n",
    "        filename += \"_weighted\"\n",
    "    if leading_lepton:\n",
    "        filename += \"_leading\"\n",
    "    if view:\n",
    "        filename += f\"_{view.lower()}\"\n",
    "    if tier is None:\n",
    "        file_suffix = \"all\"\n",
    "    else:\n",
    "        file_suffix = f\"tier_{tier}\"\n",
    "    save_plot(fig, f'{filename}_{file_suffix}', subdir=metric)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Vertexing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_node_vertex_delta(validation, percentile=68.27):\n",
    "    return (np.percentile(np.abs(validation.vtx_dx), percentile),\n",
    "            np.percentile(np.abs(validation.vtx_dy), percentile),\n",
    "            np.percentile(np.abs(validation.vtx_dz), percentile),\n",
    "            np.percentile(validation.vtx_dr, percentile))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_vertex_delta(streamed_delta, standard_delta, data_labels, delta_var, file_prefix):\n",
    "    fig, ax = plt.subplots(figsize=(12,8))\n",
    "    \n",
    "    if delta_var.lower() != \"dr\":\n",
    "        bins = np.linspace(-20.5, 20.5, 42)\n",
    "    else:\n",
    "        bins = np.linspace(0, 40, 81)\n",
    "    std_weights = np.ones_like(standard_delta)\n",
    "    std_weights /= len(standard_delta)\n",
    "    stm_weights = np.ones_like(streamed_delta)\n",
    "    stm_weights /= len(streamed_delta)\n",
    "    \n",
    "    ax.hist(streamed_delta, bins=bins, weights=stm_weights, histtype='step', label=data_labels[0])\n",
    "    ax.hist(standard_delta, bins=bins, weights=std_weights, histtype='step', label=data_labels[1])\n",
    "        \n",
    "    ax.tick_params(axis='x', labelsize=labelsize)\n",
    "    ax.tick_params(axis='y', labelsize=labelsize)\n",
    "    ax.set_xlabel(\"reco - true\", fontsize=titlesize)\n",
    "    ax.set_ylabel('fraction', fontsize=titlesize)\n",
    "    ax.set_title(f'Vertex {delta_var.lower()}', fontsize=titlesize)\n",
    "\n",
    "    ax.legend(fontsize=titlesize)\n",
    "\n",
    "    fig.tight_layout()\n",
    "    plt.show()\n",
    "    save_plot(fig, f'{file_prefix.lower()}_vertex_{delta_var.lower()}', subdir=\"vertex\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Track/Shower Id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_pid(validation, pdg, signed=False):\n",
    "    mc_pdg = abs(validation.mc_pdg) if not signed else validation.mc_pdg\n",
    "    pdg = abs(pdg) if not signed else pdg\n",
    "    correct_pid = 11 if pdg in [11, -11, 22] else 13\n",
    "    #mc_purity = np.array([val[0] if len(val) > 0 else -1 for val in validation.purity_list])\n",
    "    #mc_completeness = np.array([val[0] if len(val) > 0 else -1 for val in validation.completeness_list])\n",
    "    #mc_idx = np.where(mc_pdg == pdg)\n",
    "    #mc_idx = np.where((mc_pdg == pdg) & (validation.n_matches == 1) & (mc_purity > purity_cut) & \\\n",
    "    #                  (mc_completeness > completeness_cut))\n",
    "    mc_idx = np.where((mc_pdg == pdg) & (validation.n_matches > 0) & (validation.is_quality))   \n",
    "    good_pids = validation.reco_id_list[mc_idx].flatten()\n",
    "    return (good_pids == correct_pid).sum() / len(good_pids) if len(good_pids) else 0\n",
    "\n",
    "def get_all_pid(validation):\n",
    "    photon_pid = get_pid(validation, 22)\n",
    "    proton_pid = get_pid(validation, 2212)\n",
    "    muon_pid = get_pid(validation, 13)\n",
    "    electron_pid = get_pid(validation, 11)\n",
    "    pion_pid = get_pid(validation, 211)\n",
    "    kaon_pid = get_pid(validation, 321)\n",
    "    all_pid = np.array([muon_pid, proton_pid, pion_pid, kaon_pid, photon_pid, electron_pid])\n",
    "    return all_pid\n",
    "\n",
    "def plot_pids(all_pids, data_labels, file_prefix):\n",
    "    particle_labels = ['\\u03bc', 'p', '\\u03c0', '\\u03ba', '\\u03b3', 'e']\n",
    "    width = 0.5       # the width of the bars: can also be len(x) sequence\n",
    "    colors = ['blue', 'orange']\n",
    "\n",
    "    fig, ax = plt.subplots(figsize=(12,8))\n",
    "\n",
    "    for i, all_pid in enumerate(all_pids):\n",
    "        ax.bar(particle_labels, all_pid, width, label=data_labels[i], fill=False, edgecolor=colors[i])\n",
    "\n",
    "    ax.tick_params(axis='x', labelsize=labelsize)\n",
    "    ax.tick_params(axis='y', labelsize=labelsize)\n",
    "    ax.set_xlabel(\"particle\", fontsize=titlesize)\n",
    "    ax.set_ylabel('fraction', fontsize=titlesize)\n",
    "    ax.set_title('Correct PID', fontsize=titlesize)\n",
    "\n",
    "    ax.legend(fontsize=titlesize, loc='upper center')\n",
    "\n",
    "    fig.tight_layout()\n",
    "    plt.show()\n",
    "    save_plot(fig, f'{file_prefix}_pid', subdir=\"pid\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_pid(filenames, labels, treename):\n",
    "    all_pids = []\n",
    "    for filename in filenames:\n",
    "        validation = MCValidation(filename, treename)\n",
    "        all_pids.append(get_all_pid(validation))\n",
    "    file_prefix = \"\"\n",
    "    if \"unfolded\" in filename:\n",
    "        file_prefix += \"unfolded\"\n",
    "    else:\n",
    "        file_prefix += \"folded\"\n",
    "    plot_pids(all_pids, labels, file_prefix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_vertex(filenames, labels, treename):\n",
    "    all_vtx = []\n",
    "    for filename in filenames:\n",
    "        validation = MCValidation(filename, treename)\n",
    "        all_vtx.append(validation)\n",
    "\n",
    "    for i in range(len(labels)):\n",
    "        vtx_deltas_68 = get_node_vertex_delta(all_vtx[i])\n",
    "        print(f\"{labels[i]} node vertex deltas: x = {vtx_deltas_68[0]:.2} y = {vtx_deltas_68[1]:.2} z = {vtx_deltas_68[2]:.2} r = {vtx_deltas_68[3]:.2}\")\n",
    "    \n",
    "    plot_vertex_delta(all_vtx[0].vtx_dx, all_vtx[1].vtx_dx, labels, \"dx\", \"node\")\n",
    "    plot_vertex_delta(all_vtx[0].vtx_dy, all_vtx[1].vtx_dy, labels, \"dy\", \"node\")\n",
    "    plot_vertex_delta(all_vtx[0].vtx_dz, all_vtx[1].vtx_dz, labels, \"dz\", \"node\")\n",
    "    plot_vertex_delta(all_vtx[0].vtx_dr, all_vtx[1].vtx_dr, labels, \"dr\", \"node\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hierarchy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_hierarchy_efficiency(validation, pdg, tier, signed=False):\n",
    "    mc_pdg = abs(validation.mc_pdg) if not signed else validation.mc_pdg\n",
    "    pdg = abs(pdg) if not signed else pdg\n",
    "    particle_filter = mc_pdg == pdg\n",
    "    particle_filter = particle_filter & (validation.mc_tier == tier)\n",
    "    particle_filter = particle_filter & (validation.n_expected_children > 0)\n",
    "    particle_filter = particle_filter & (validation.expected_children_adc_list.sum() > 0)\n",
    "\n",
    "    mc_idx = np.where((particle_filter) & (validation.is_good_match))\n",
    "    \n",
    "    n_reco_children = validation.good_reco_children_list[mc_idx]\n",
    "    adc_exp_children = validation.expected_children_adc_list[mc_idx]\n",
    "    numerator = (n_reco_children * adc_exp_children).sum()\n",
    "    denomenator = adc_exp_children.sum()\n",
    "\n",
    "    return numerator / denomenator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_hierarchy_efficiency(efficiency_1, efficiency_2, pdg, data_labels):\n",
    "    particle_labels = {13: '\\u03bc', 2212: 'p', 211: '\\u03c0', 321: '\\u03ba', 22: '\\u03b3', 11: 'e'}\n",
    "    file_labels = {13: 'muon', 2212: 'proton', 211: 'pion', 321: 'kaon', 22: 'gamma', 11: 'electron'}\n",
    "    colors = ['blue', 'orange']\n",
    "\n",
    "    fig, ax = plt.subplots(figsize=(12,8))\n",
    "\n",
    "    bins = np.linspace(0, 1, 21)\n",
    "    weights1 = np.ones_like(efficiency_1) / len(efficiency_1)\n",
    "    weights2 = np.ones_like(efficiency_2) / len(efficiency_2)\n",
    "    \n",
    "    ax.hist(efficiency_1, bins=bins, weights=weights1, label=data_labels[0], edgecolor=colors[0], histtype='step')\n",
    "    ax.hist(efficiency_2, bins=bins, weights=weights2, label=data_labels[1], edgecolor=colors[1], histtype='step')\n",
    "\n",
    "    ax.tick_params(axis='x', labelsize=labelsize)\n",
    "    ax.tick_params(axis='y', labelsize=labelsize)\n",
    "    ax.set_xlabel(\"efficiency\", fontsize=titlesize)\n",
    "    ax.set_ylabel('fraction', fontsize=titlesize)\n",
    "    ax.set_title(f'{particle_labels[pdg]} hierarchy efficiency', fontsize=titlesize)\n",
    "\n",
    "    ax.legend(fontsize=titlesize)\n",
    "\n",
    "    fig.tight_layout()\n",
    "    plt.show()\n",
    "    save_plot(fig, f'hier_eff_{file_labels[pdg]}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Node level execution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_streamed = MCValidation(\"stm/hierarchy_validation_mc.root\", \"mc\")\n",
    "val_standard = MCValidation(\"std/hierarchy_validation_mc.root\", \"mc\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(np.unique(val_standard.event))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(np.unique(val_streamed.event))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "original_cycler = mpl.rcParams['axes.prop_cycle']\n",
    "mpl.rcParams['axes.prop_cycle'] = cycler(color=['#2ca02c', '#1f77b4', '#ff7f0e', '#d62728', '#9467bd',\n",
    "                                                '#8c564b', '#e377c2', '#7f7f7f', '#bcbd22', '#17becf'])\n",
    "\n",
    "for tier in [ None, 1 ]:\n",
    "    run_eff(\"std/hierarchy_validation_mc.root\", \"mc\", tier=tier)\n",
    "    run_eff(\"stm/hierarchy_validation_mc.root\", \"mc\", tier=tier)\n",
    "\n",
    "mpl.rcParams['axes.prop_cycle'] = original_cycler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for tier in [ None, 1 ]:\n",
    "    ps_streamed = get_all_purities(val_streamed, tier=tier)\n",
    "    ps_standard = get_all_purities(val_standard, tier=tier)\n",
    "    cs_streamed = get_all_completenesses(val_streamed, tier=tier)\n",
    "    cs_standard = get_all_completenesses(val_standard, tier=tier)\n",
    "    pcs_streamed = get_all_pc_metrics(val_streamed, tier=tier)\n",
    "    pcs_standard = get_all_pc_metrics(val_standard, tier=tier)\n",
    "    for p, particle in enumerate(['muon', 'proton', 'pion', 'kaon', 'photon', 'electron']):\n",
    "        plot_metric(ps_streamed[p], ps_standard[p], [\"streamed\", \"standard\"], particle=particle, metric=\"purity\", tier=tier, log=False)\n",
    "        plot_metric(cs_streamed[p], cs_standard[p], [\"streamed\", \"standard\"], particle=particle, metric=\"completeness\", tier=tier, log=False)\n",
    "        plot_metric(pcs_streamed[p], pcs_standard[p], [\"streamed\", \"standard\"], particle=particle, metric=\"purity_completeness\", tier=tier, log=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for tier in [ None, 1 ]:\n",
    "    ps_streamed = get_all_purities(val_streamed, weighted=True, tier=tier)\n",
    "    ps_standard = get_all_purities(val_standard, weighted=True, tier=tier)\n",
    "    cs_streamed = get_all_completenesses(val_streamed, weighted=True, tier=tier)\n",
    "    cs_standard = get_all_completenesses(val_standard, weighted=True, tier=tier)\n",
    "    pcs_streamed = get_all_pc_metrics(val_streamed, weighted=True, tier=tier)\n",
    "    pcs_standard = get_all_pc_metrics(val_standard, weighted=True, tier=tier)\n",
    "    for p, particle in enumerate(['muon', 'proton', 'pion', 'kaon', 'photon', 'electron']):\n",
    "        plot_metric(ps_streamed[p], ps_standard[p], [\"streamed\", \"standard\"], particle=particle, metric=\"purity\", weighted=True, tier=tier, log=False)\n",
    "        plot_metric(cs_streamed[p], cs_standard[p], [\"streamed\", \"standard\"], particle=particle, metric=\"completeness\", weighted=True, tier=tier, log=False)\n",
    "        plot_metric(pcs_streamed[p], pcs_standard[p], [\"streamed\", \"standard\"], particle=particle, metric=\"purity_completeness\", weighted=True, tier=tier, log=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filenames = [\"stm/hierarchy_validation_mc.root\", \"std/hierarchy_validation_mc.root\"]\n",
    "labels = [\"streamed\", \"standard\"]\n",
    "run_pid(filenames, labels, \"mc\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filenames = [\"stm/hierarchy_validation_mc.root\", \"std/hierarchy_validation_mc.root\"]\n",
    "labels = [\"streamed\", \"standard\"]\n",
    "run_vertex(filenames, labels, \"mc\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hierarchy efficiency\n",
    "\n",
    "Not currently implemented"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#pdg = 2212\n",
    "#hier_eff_streamed = get_hierarchy_efficiency(val_streamed, pdg, 1)\n",
    "#hier_eff_standard = get_hierarchy_efficiency(val_standard, pdg, 1)\n",
    "#plot_hierarchy_efficiency(hier_eff_streamed, hier_eff_standard, pdg, [\"streamed\", \"standard\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Event-level validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EventValidation:\n",
    "    def __init__(self, filename, treename):\n",
    "        file = uproot.open(filename)\n",
    "        tree = file[treename]\n",
    "        self.event = tree.array(\"event\")\n",
    "        self.orig_event = tree.array(\"event\")\n",
    "        self.file = np.zeros_like(self.event)\n",
    "        self.interaction_type = tree.array(\"interactionType\")\n",
    "        self.n_good_matches = tree.array(\"nGoodMatches\")\n",
    "        self.n_above_threshold_matches = tree.array(\"nPoorMatches\")\n",
    "        self.n_unmatched = tree.array(\"nUnmatched\")\n",
    "        self.n_nodes = tree.array(\"nNodes\")\n",
    "        self.n_good_tier1_matches = tree.array(\"nGoodTier1Matches\")\n",
    "        self.n_tier1_nodes = tree.array(\"nTier1Nodes\")\n",
    "        self.n_good_track_matches = tree.array(\"nGoodTrackMatches\")\n",
    "        self.n_track_nodes = tree.array(\"nTrackNodes\")\n",
    "        self.n_good_tier1_track_matches = tree.array(\"nGoodTier1TrackMatches\")\n",
    "        self.n_tier1_track_nodes = tree.array(\"nTier1TrackNodes\")        \n",
    "        self.n_good_shower_matches = tree.array(\"nGoodShowerMatches\")\n",
    "        self.n_shower_nodes = tree.array(\"nShowerNodes\")\n",
    "        self.n_good_tier1_shower_matches = tree.array(\"nGoodTier1ShowerMatches\")\n",
    "        self.n_tier1_shower_nodes = tree.array(\"nTier1ShowerNodes\")        \n",
    "        self.has_leading_muon = tree.array(\"hasLeadingMuon\")\n",
    "        self.has_leading_electron = tree.array(\"hasLeadingElectron\")\n",
    "        self.is_leading_lepton_correct = tree.array(\"isLeadingLeptonCorrect\")\n",
    "        self.nu_vtx_dx = tree.array(\"vtxDx\")\n",
    "        self.nu_vtx_dy = tree.array(\"vtxDy\")\n",
    "        self.nu_vtx_dz = tree.array(\"vtxDz\")\n",
    "        self.nu_vtx_dr = tree.array(\"vtxDr\")\n",
    "        file.close()\n",
    "        make_sequential(self)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_correct_event_fraction(validation, fltr=None):\n",
    "    if not fltr:\n",
    "        idx = np.where(validation.n_nodes > 0)\n",
    "    else:\n",
    "        idx = np.where((validation.n_nodes > 0) & (np.in1d(validation.interaction_type, fltr)))\n",
    "    good = (validation.n_good_matches[idx] == validation.n_nodes[idx]).sum(dtype=np.float)\n",
    "    n = (validation.n_nodes[idx] > 0).sum(dtype=np.float)\n",
    "    err = np.sqrt((good * (n - good)) / (n**3))\n",
    "    f = good / n\n",
    "    return f, err\n",
    "\n",
    "def get_event_correctness(validation):\n",
    "    idx = np.where(validation.n_nodes > 0)\n",
    "    return validation.n_good_matches[idx] / validation.n_nodes[idx]\n",
    "\n",
    "def get_correct_event_fraction_by_n_primaries(validation):\n",
    "    unique_node_count = np.unique(validation.n_nodes)\n",
    "    correct_fractions = {}\n",
    "    for node_count in unique_node_count:\n",
    "        idx = np.where(validation.n_nodes == node_count)\n",
    "        correct_fractions[node_count] = (validation.n_good_matches[idx] == validation.n_nodes[idx]).mean()\n",
    "    \n",
    "    return correct_fractions\n",
    "\n",
    "def get_correct_tier1_fraction(validation, fltr=None):\n",
    "    if not fltr:\n",
    "        idx = np.where(validation.n_tier1_nodes > 0)\n",
    "    else:\n",
    "        idx = np.where((validation.n_tier1_nodes > 0) & (np.in1d(validation.interaction_type, fltr)))\n",
    "    good = (validation.n_good_tier1_matches[idx] == validation.n_tier1_nodes[idx]).sum(dtype=np.float)\n",
    "    n = (validation.n_tier1_nodes[idx] > 0).sum(dtype=np.float)\n",
    "    err = np.sqrt(good * (n - good) / n**3)\n",
    "    f = good / n\n",
    "    return f, err\n",
    "\n",
    "def get_tier1_correctness(validation):\n",
    "    idx = np.where(validation.n_tier1_nodes > 0)\n",
    "    return validation.n_good_tier1_matches[idx] / validation.n_tier1_nodes[idx]\n",
    "\n",
    "def get_correct_leading_lepton_fraction(validation):\n",
    "    if np.any(validation.has_leading_muon) or np.any(validation.has_leading_electron):\n",
    "        correct = validation.is_leading_lepton_correct.sum()\n",
    "        n = validation.has_leading_muon.sum() + validation.has_leading_electron.sum()\n",
    "        f = correct / n\n",
    "        err = np.sqrt(f) / n\n",
    "        return f, err\n",
    "    else:\n",
    "        return 0., 0.\n",
    "\n",
    "def get_correct_leading_muon_fraction(validation):\n",
    "    if np.any(validation.has_leading_muon):\n",
    "        correct = validation.is_leading_lepton_correct[np.where(validation.has_leading_muon)].sum()\n",
    "        n = validation.has_leading_muon.sum()\n",
    "        f = correct / n\n",
    "        err = np.sqrt(f) / n\n",
    "        return f, err\n",
    "    else:\n",
    "        return 0., 0.\n",
    "\n",
    "def get_correct_leading_electron_fraction(validation):\n",
    "    if np.any(validation.has_leading_electron):\n",
    "        correct = validation.is_leading_lepton_correct[np.where(validation.has_leading_electron)].sum()\n",
    "        n = validation.has_leading_electron.sum()\n",
    "        f = correct / n\n",
    "        err = np.sqrt(f) / n\n",
    "        return f, err\n",
    "    else:\n",
    "        return 0., 0.\n",
    "\n",
    "def get_above_threshold_event_fraction(validation):\n",
    "    return ((validation.n_good_matches + validation.n_above_threshold_matches) == validation.n_nodes).sum() \\\n",
    "        / (validation.n_nodes > 0).sum()\n",
    "\n",
    "def get_above_threshold_event_fraction_by_n_primaries(validation):\n",
    "    unique_node_count = np.unique(validation.n_nodes)\n",
    "    correct_fractions = {}\n",
    "    for node_count in unique_node_count:\n",
    "        idx = np.where(validation.n_nodes == node_count)\n",
    "        correct_fractions[node_count] = ((validation.n_good_matches[idx] + validation.n_above_threshold_matches[idx]) \\\n",
    "                                         == validation.n_nodes[idx]).mean()\n",
    "    \n",
    "    return correct_fractions\n",
    "\n",
    "def get_correct_tracks_fraction(validation):\n",
    "    idx = np.where(validation.n_track_nodes > 0)\n",
    "    return (validation.n_good_track_matches[idx] == validation.n_track_nodes[idx]).sum() / (validation.n_track_nodes > 0).sum()\n",
    "\n",
    "def get_correct_tracks_fraction_by_n_tracks(validation):\n",
    "    unique_node_count = np.unique(validation.n_track_nodes)\n",
    "    correct_fractions = {}\n",
    "    for node_count in unique_node_count:\n",
    "        if not node_count: continue\n",
    "        idx = np.where(validation.n_track_nodes == node_count)\n",
    "        correct_fractions[node_count] = (validation.n_good_track_matches[idx] == validation.n_track_nodes[idx]).mean()\n",
    "    \n",
    "    return correct_fractions\n",
    "\n",
    "def get_fraction_tracks_matched(validation):\n",
    "    idx = np.where(validation.n_track_nodes > 0)\n",
    "    return (validation.n_good_track_matches[idx] / validation.n_track_nodes[idx]).mean()\n",
    "\n",
    "def get_correct_tier1_tracks_fraction(validation):\n",
    "    idx = np.where(validation.n_tier1_track_nodes > 0)\n",
    "    return (validation.n_good_tier1_track_matches[idx] == validation.n_tier1_track_nodes[idx]).sum() / (validation.n_tier1_track_nodes > 0).sum()\n",
    "\n",
    "def get_fraction_tier1_tracks_matched(validation):\n",
    "    idx = np.where(validation.n_tier1_track_nodes > 0)\n",
    "    return (validation.n_good_tier1_track_matches[idx] / validation.n_tier1_track_nodes[idx]).mean()\n",
    "\n",
    "def get_correct_showers_fraction(validation):\n",
    "    idx = np.where(validation.n_shower_nodes > 0)\n",
    "    return (validation.n_good_shower_matches[idx] == validation.n_shower_nodes[idx]).sum() / (validation.n_shower_nodes > 0).sum()\n",
    "\n",
    "def get_correct_showers_fraction_by_n_showers(validation):\n",
    "    unique_node_count = np.unique(validation.n_shower_nodes)\n",
    "    correct_fractions = {}\n",
    "    for node_count in unique_node_count:\n",
    "        if not node_count: continue\n",
    "        idx = np.where(validation.n_shower_nodes == node_count)\n",
    "        correct_fractions[node_count] = (validation.n_good_shower_matches[idx] == validation.n_shower_nodes[idx]).mean()\n",
    "    \n",
    "    return correct_fractions\n",
    "\n",
    "def get_correct_tier1_showers_fraction(validation):\n",
    "    idx = np.where(validation.n_tier1_shower_nodes > 0)\n",
    "    return (validation.n_good_tier1_shower_matches[idx] == validation.n_tier1_shower_nodes[idx]).sum() / (validation.n_tier1_shower_nodes > 0).sum()\n",
    "\n",
    "def get_fraction_showers_matched(validation):\n",
    "    idx = np.where(validation.n_shower_nodes > 0)\n",
    "    return (validation.n_good_shower_matches[idx] / validation.n_shower_nodes[idx]).mean()\n",
    "\n",
    "def get_fraction_tier1_showers_matched(validation):\n",
    "    idx = np.where(validation.n_tier1_shower_nodes > 0)\n",
    "    return (validation.n_good_tier1_shower_matches[idx] / validation.n_tier1_shower_nodes[idx]).mean()\n",
    "\n",
    "def get_fraction_nodes_matched(validation):\n",
    "    idx = np.where(validation.n_nodes > 0)\n",
    "    return (validation.n_good_matches[idx] / validation.n_nodes[idx]).mean()\n",
    "\n",
    "def get_fraction_nodes_matched_by_n_primaries(validation):\n",
    "    idx = np.where(validation.n_nodes > 0)\n",
    "    unique_node_count = np.unique(validation.n_nodes[idx])\n",
    "    fraction_matched = {}\n",
    "    for node_count in unique_node_count:\n",
    "        idx = np.where(validation.n_nodes == node_count)\n",
    "        fraction_matched[node_count] = (validation.n_good_matches[idx] / validation.n_nodes[idx]).mean()\n",
    "    \n",
    "    return fraction_matched\n",
    "\n",
    "def get_neutrino_vertex_delta(validation, percentile=68.27):\n",
    "    return (np.percentile(np.abs(validation.nu_vtx_dx), percentile),\n",
    "            np.percentile(np.abs(validation.nu_vtx_dy), percentile),\n",
    "            np.percentile(np.abs(validation.nu_vtx_dz), percentile),\n",
    "            np.percentile(validation.nu_vtx_dr, percentile))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_event_correctness(streamed_correctness, standard_correctness, data_labels, file_prefix):\n",
    "    fig, ax = plt.subplots(figsize=(12,8))\n",
    "\n",
    "    bins = np.linspace(0, 1, 21)\n",
    "    std_weights = np.ones_like(standard_correctness)\n",
    "    std_weights /= len(standard_correctness)\n",
    "    stm_weights = np.ones_like(streamed_correctness)\n",
    "    stm_weights /= len(streamed_correctness)\n",
    "    \n",
    "    ax.hist(standard_correctness, bins=bins, weights=std_weights, histtype='step', label=data_labels[0])\n",
    "    ax.hist(streamed_correctness, bins=bins, weights=stm_weights, histtype='step', label=data_labels[1])\n",
    "    \n",
    "    ax.tick_params(axis='x', labelsize=labelsize)\n",
    "    ax.tick_params(axis='y', labelsize=labelsize)\n",
    "    ax.set_xlabel(\"correct primaries fraction\", fontsize=titlesize)\n",
    "    ax.set_ylabel('fraction', fontsize=titlesize)\n",
    "    ax.set_title('Correct Primaries Fraction', fontsize=titlesize)\n",
    "\n",
    "    ax.legend(fontsize=titlesize, loc='upper left')\n",
    "\n",
    "    fig.tight_layout()\n",
    "    plt.show()\n",
    "    save_plot(fig, f'{file_prefix}_event_correctness', subdir=\"event\")\n",
    "\n",
    "def plot_event_correctness_with_errors(streamed_correctness, standard_correctness, data_labels, file_prefix):\n",
    "    fig, ax = plt.subplots(figsize=(12,8))\n",
    "\n",
    "    bins = np.linspace(0, 1, 21)\n",
    "    weights = np.ones_like(streamed_correctness)\n",
    "    weights /= len(streamed_correctness)\n",
    "    data1, _ = np.histogram(streamed_correctness, bins=bins)\n",
    "    data2, _ = np.histogram(standard_correctness, bins=bins)\n",
    "\n",
    "    bin_centres = 0.5*(bins[1:] + bins[:-1])\n",
    "    err1 = np.sqrt(data1) / len(streamed_correctness)\n",
    "    err2 = np.sqrt(data2) / len(standard_correctness)\n",
    "    data1 = data1 / len(streamed_correctness)\n",
    "    data2 = data2 / len(standard_correctness)\n",
    "    width = 0.05\n",
    "    \n",
    "    ax.errorbar(bin_centres, data1, yerr=err1, marker='.', drawstyle='steps-mid', label=data_labels[0])\n",
    "    ax.errorbar(bin_centres, data2, yerr=err2, marker='.', drawstyle='steps-mid', label=data_labels[1])\n",
    "    \n",
    "    ax.tick_params(axis='x', labelsize=labelsize)\n",
    "    ax.tick_params(axis='y', labelsize=labelsize)\n",
    "    ax.set_xlabel(\"Correct primaries fraction\", fontsize=titlesize)\n",
    "    ax.set_ylabel('Event fraction', fontsize=titlesize)\n",
    "    ax.set_title('Correct Primaries Fraction', fontsize=titlesize)\n",
    "\n",
    "    ax.legend(fontsize=titlesize)\n",
    "\n",
    "    fig.tight_layout()\n",
    "    plt.show()\n",
    "    save_plot(fig, f'{file_prefix}_event_correctness')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "interaction_dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_event_level(filename, treename):\n",
    "    validation = EventValidation(filename, treename)\n",
    "    # nodes\n",
    "    int_type_fltr = None#[200]\n",
    "    correct_event_fraction, cef_err = get_correct_event_fraction(validation, fltr=int_type_fltr)\n",
    "    correct_tier1_fraction, ctf_err = get_correct_tier1_fraction(validation, fltr=int_type_fltr)\n",
    "    correct_leading_lepton_fraction, cllf_err = get_correct_leading_lepton_fraction(validation)\n",
    "    correct_leading_muon_fraction, clmf_err = get_correct_leading_muon_fraction(validation)\n",
    "    correct_leading_electron_fraction, clef_err = get_correct_leading_electron_fraction(validation)\n",
    "    correct_event_fraction_by_n_primaries = get_correct_event_fraction_by_n_primaries(validation)\n",
    "    correct_event_fraction_by_n_primaries = {k: correct_event_fraction_by_n_primaries[k] for\n",
    "                                             k in correct_event_fraction_by_n_primaries if k < 16}\n",
    "    almost_correct_fraction = get_above_threshold_event_fraction(validation)\n",
    "    node_fraction = get_fraction_nodes_matched(validation)\n",
    "    node_fraction_by_n_primaries = get_fraction_nodes_matched_by_n_primaries(validation)\n",
    "    node_fraction_by_n_primaries = {k: node_fraction_by_n_primaries[k] for k in node_fraction_by_n_primaries if k < 16}\n",
    "    print(f\"Correct event fraction: {correct_event_fraction:.3} +- {cef_err:.1}\")\n",
    "    print(f\"Correct tier 1 fraction: {correct_tier1_fraction:.3} +- {ctf_err:.1}\")\n",
    "    print(f\"Correct leading lepton fraction: {correct_leading_lepton_fraction:.2} +- {cllf_err:.1} mu: {correct_leading_muon_fraction:.2} +- {clmf_err:.1} e: {correct_leading_electron_fraction:.2} +- {clef_err:.1}\")\n",
    "    print(f\"Correct event fraction by N primaries: \", end=\"\")\n",
    "    for n_primaries in correct_event_fraction_by_n_primaries:\n",
    "        print(f\"{n_primaries}: {correct_event_fraction_by_n_primaries[n_primaries]:.2}\", end=\", \")\n",
    "    print()\n",
    "    print(f\"Almost correct event fraction: {almost_correct_fraction:.2}\")\n",
    "    print(f\"Fraction of nodes reconstructed: {node_fraction:.2}\")\n",
    "    print(f\"Fraction of nodes reconstructed by N primaries: \")\n",
    "    for n_primaries in node_fraction_by_n_primaries:\n",
    "        print(f\"{n_primaries}: {node_fraction_by_n_primaries[n_primaries]:.2}\", end=\", \")\n",
    "    print()\n",
    "    \n",
    "    # tracks\n",
    "    correct_tracks_fraction = get_correct_tracks_fraction(validation)\n",
    "    correct_tier1_tracks_fraction = get_correct_tier1_tracks_fraction(validation)\n",
    "    correct_tracks_fraction_by_n_primaries = get_correct_tracks_fraction_by_n_tracks(validation)\n",
    "    correct_tracks_fraction_by_n_primaries = {k: correct_tracks_fraction_by_n_primaries[k] for\n",
    "                                              k in correct_tracks_fraction_by_n_primaries if k < 16}\n",
    "    track_fraction = get_fraction_tracks_matched(validation)\n",
    "    tier1_track_fraction = get_fraction_tier1_tracks_matched(validation)\n",
    "    print(f\"Correct tracks fraction: {correct_tracks_fraction:.2}\")\n",
    "    print(f\"Correct tier1 tracks fraction: {correct_tier1_tracks_fraction:.2}\")\n",
    "    print(f\"Correct tracks fraction by N primaries: \")\n",
    "    for n_primaries in correct_tracks_fraction_by_n_primaries:\n",
    "        print(f\"{n_primaries}: {correct_tracks_fraction_by_n_primaries[n_primaries]:.2}\", end=\", \")\n",
    "    print()\n",
    "    print(f\"Fraction of tracks reconstructed: {track_fraction:.2}\")\n",
    "    print(f\"Fraction of tier1 tracks reconstructed: {tier1_track_fraction:.2}\")\n",
    "    \n",
    "    # showers\n",
    "    correct_showers_fraction = get_correct_showers_fraction(validation)\n",
    "    correct_tier1_showers_fraction = get_correct_tier1_showers_fraction(validation)\n",
    "    correct_showers_fraction_by_n_primaries = get_correct_showers_fraction_by_n_showers(validation)\n",
    "    correct_showers_fraction_by_n_primaries = {k: correct_showers_fraction_by_n_primaries[k] for\n",
    "                                               k in correct_showers_fraction_by_n_primaries if k < 16}\n",
    "    shower_fraction = get_fraction_showers_matched(validation)\n",
    "    tier1_shower_fraction = get_fraction_tier1_showers_matched(validation)\n",
    "    print(f\"Correct showers fraction: {correct_showers_fraction:.2}\")\n",
    "    print(f\"Correct tier1 showers fraction: {correct_tier1_showers_fraction:.2}\")\n",
    "    print(f\"Correct showers fraction by N primaries: \")\n",
    "    for n_primaries in correct_showers_fraction_by_n_primaries:\n",
    "        print(f\"{n_primaries}: {correct_showers_fraction_by_n_primaries[n_primaries]:.2}\", end=\", \")\n",
    "    print()\n",
    "    print(f\"Fraction of showers reconstructed: {shower_fraction:.2}\")\n",
    "    print(f\"Fraction of tier1 showers reconstructed: {tier1_shower_fraction:.2}\")\n",
    "    \n",
    "    vtx_deltas_68 = get_neutrino_vertex_delta(validation)\n",
    "    print(f\"nu vertex deltas: x = {vtx_deltas_68[0]:.2} y = {vtx_deltas_68[1]:.2} z = {vtx_deltas_68[2]:.2} r = {vtx_deltas_68[3]:.2}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Standard\")\n",
    "run_event_level(\"std/hierarchy_validation_event.root\", \"events\")\n",
    "print(\"Streamed\")\n",
    "run_event_level(\"stm/hierarchy_validation_event.root\", \"events\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "stm_evt_validation = EventValidation(\"stm/hierarchy_validation_event.root\", \"events\")\n",
    "std_evt_validation = EventValidation(\"std/hierarchy_validation_event.root\", \"events\")\n",
    "streamed_event_correctness = get_event_correctness(stm_evt_validation)\n",
    "standard_event_correctness = get_event_correctness(std_evt_validation)\n",
    "plot_event_correctness(streamed_event_correctness, standard_event_correctness, [\"streamed\", \"standard\"], \"folded\")\n",
    "plot_vertex_delta(stm_evt_validation.nu_vtx_dx, std_evt_validation.nu_vtx_dx, [\"streamed\", \"standard\"], \"dx\", \"nu\")\n",
    "plot_vertex_delta(stm_evt_validation.nu_vtx_dy, std_evt_validation.nu_vtx_dy, [\"streamed\", \"standard\"], \"dy\", \"nu\")\n",
    "plot_vertex_delta(stm_evt_validation.nu_vtx_dz, std_evt_validation.nu_vtx_dz, [\"streamed\", \"standard\"], \"dz\", \"nu\")\n",
    "plot_vertex_delta(stm_evt_validation.nu_vtx_dr, std_evt_validation.nu_vtx_dr, [\"streamed\", \"standard\"], \"dr\", \"nu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Misc cross checks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def identify_broken(baseline, feature, file_id):\n",
    "    idx_correct_base = np.where((baseline.file == file_id) & (baseline.n_good_tier1_matches == baseline.n_tier1_nodes))\n",
    "    evt_correct_base = baseline.orig_event[idx_correct_base]\n",
    "    idx_incorrect_feat = np.where((feature.file == file_id) & (feature.n_good_tier1_matches != feature.n_tier1_nodes))\n",
    "    evt_incorrect_feat = feature.orig_event[idx_incorrect_feat]\n",
    "    \n",
    "    return np.intersect1d(evt_correct_base, evt_incorrect_feat)\n",
    "\n",
    "def identify_fixed(baseline, feature, file_id):\n",
    "    idx_incorrect_base = np.where((baseline.file == file_id) & (baseline.n_good_tier1_matches != baseline.n_tier1_nodes))\n",
    "    evt_incorrect_base = baseline.orig_event[idx_incorrect_base]\n",
    "    idx_correct_feat = np.where((feature.file == file_id) & (feature.n_good_tier1_matches == feature.n_tier1_nodes))\n",
    "    evt_correct_feat = feature.orig_event[idx_correct_feat]\n",
    "    \n",
    "    return np.intersect1d(evt_incorrect_base, evt_correct_feat)\n",
    "\n",
    "\n",
    "def identify_missing(baseline, feature, file_id):\n",
    "    idx_base = np.where(baseline.file == file_id)\n",
    "    evt_base = baseline.orig_event[idx_base]\n",
    "    idx_feat = np.where(feature.file == file_id)\n",
    "    evt_feat = feature.orig_event[idx_feat]\n",
    "    \n",
    "    return list(set(evt_base).difference(evt_feat))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "identify_broken(std_evt_validation, stm_evt_validation, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "identify_fixed(std_evt_validation, stm_evt_validation, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "broken = fixed = 0\n",
    "files = np.unique(std_evt_validation.file)\n",
    "for fid in files:\n",
    "    broken += len(identify_broken(std_evt_validation, stm_evt_validation, fid))\n",
    "    fixed += len(identify_fixed(std_evt_validation, stm_evt_validation, 0))\n",
    "print(f\"Broken: {broken} Fixed: {fixed} Events: {len(np.unique(std_evt_validation.event))}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "summary_missing = {}\n",
    "for fid in files:\n",
    "    summary_missing[fid] = identify_missing(std_evt_validation, stm_evt_validation, fid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "summary_missing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "missing_list = list(summary_missing.values())\n",
    "flat_list = [item for sublist in missing_list for item in sublist]\n",
    "len(flat_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "summary_broken = {}\n",
    "for fid in files:\n",
    "    summary_broken[fid] = identify_broken(std_evt_validation, stm_evt_validation, fid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "summary_broken"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "idx = np.where((std_evt_validation.file == 0) & (std_evt_validation.orig_event == 29))\n",
    "print(std_evt_validation.n_good_tier1_matches[idx], std_evt_validation.n_tier1_nodes[idx])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "idx = np.where((stm_evt_validation.file == 0) & (stm_evt_validation.orig_event == 29))\n",
    "print(stm_evt_validation.n_good_tier1_matches[idx], stm_evt_validation.n_tier1_nodes[idx])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_streamed.purity_list_w[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_streamed.completeness_list_w[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_streamed.mc_id[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_streamed.mc_pdg[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_streamed.purity_list_w[0] * val_streamed.completeness_list_w[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_streamed.purity_list_w"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_streamed.completeness_list_w"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#This code returns specificed parameters of root files and includes some other attributes\n",
    "file_energy = 14 #In MeV, which energy file should be read\n",
    "file_criteria = \"reco_nhits_list\" #From list of keys above\n",
    "\n",
    "full_criteria = eval(\"validation.\" + file_criteria)\n",
    "size_criteria = np.size(full_criteria)\n",
    "validation = MCValidation(\"hierarchy_validation_MC_\" + str(file_energy) + \"MeV.root\", \"events\")\n",
    "\n",
    "def make_linear(validation):\n",
    "    temp_array = []\n",
    "    for i in range(0, size_criteria - 1):\n",
    "        subdivision_size = np.size(validation[i])\n",
    "        x = 1\n",
    "        temp_array.append(validation[i])\n",
    "        if subdivision_size != 1:\n",
    "            while x < subdivision_size:\n",
    "                temp_array.append(validation[i][x])\n",
    "                x += 1\n",
    "    size = np.size(temp_array)\n",
    "    return temp_array, size\n",
    "\n",
    "#size_difference = int(make_linear(full_criteria)[1]) - int(size_criteria)\n",
    "    \n",
    "\n",
    "print(\"\\033[1m\" + file_criteria + \"\\033[0m\" + \" is as follows:\")\n",
    "print(full_criteria)\n",
    "print(\"\\n\")\n",
    "print(\"The length of \" + file_criteria + \" is \" + \"\\033[1m\" + str(size_criteria) + \"\\033[0m\")\n",
    "print(\"The number of empty events recorded \" + \"\\033[1m\" + str(abs(size_difference)) + \"\\033[0m\" )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
